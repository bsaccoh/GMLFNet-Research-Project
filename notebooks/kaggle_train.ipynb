{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GMLFNet: Training on Kaggle\n",
    "\n",
    "**Gradient-Based Meta-Learning with Fast Adaptation Weights for Robust Multi-Centre Polyp Segmentation**\n",
    "\n",
    "## Setup Instructions\n",
    "1. **Enable GPU**: Settings → Accelerator → GPU T4 x2\n",
    "2. **Enable Internet**: Settings → Internet → On (required for dataset download & pretrained weights)\n",
    "3. **Run All**: Click \"Run All\" to execute the full training pipeline\n",
    "\n",
    "## What this notebook does\n",
    "1. Installs dependencies (learn2learn, albumentations, timm, etc.)\n",
    "2. Downloads polyp segmentation datasets from Google Drive\n",
    "3. Creates all GMLFNet source modules inline\n",
    "4. Builds the model and verifies architecture\n",
    "5. Runs MAML meta-learning training (200 epochs)\n",
    "6. Evaluates on test centers (ETIS-LaribPolypDB, CVC-300)\n",
    "7. Saves results and checkpoints to Kaggle output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1: Install Dependencies\n",
    "!pip install -q learn2learn albumentations timm gdown opencv-python-headless pyyaml tqdm wandb\n",
    "print(\"Dependencies installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Download and Organize Datasets\n",
    "import os\n",
    "import shutil\n",
    "import zipfile\n",
    "from pathlib import Path\n",
    "\n",
    "import gdown\n",
    "\n",
    "DATASET_ROOT = Path(\"/kaggle/working/datasets\")\n",
    "DATASET_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "DATASET_URLS = {\n",
    "    \"TrainDataset\": \"https://drive.google.com/uc?id=1lODorfB33jbd-im-qrtUgWnZXxB94F55\",\n",
    "    \"TestDataset\": \"https://drive.google.com/uc?id=1o8OfBvYE6K-EpDyvzsmMPndnUMwb540R\",\n",
    "}\n",
    "\n",
    "for name, url in DATASET_URLS.items():\n",
    "    zip_path = DATASET_ROOT / f\"{name}.zip\"\n",
    "    extract_dir = DATASET_ROOT / name\n",
    "    \n",
    "    if not zip_path.exists():\n",
    "        print(f\"Downloading {name}...\")\n",
    "        gdown.download(url, str(zip_path), quiet=False)\n",
    "    else:\n",
    "        print(f\"{name}.zip already exists, skipping download.\")\n",
    "    \n",
    "    if not extract_dir.exists():\n",
    "        print(f\"Extracting {name}...\")\n",
    "        with zipfile.ZipFile(zip_path, \"r\") as zf:\n",
    "            zf.extractall(str(DATASET_ROOT))\n",
    "        print(f\"Extracted to {extract_dir}\")\n",
    "\n",
    "# Organize per-center folders\n",
    "centers = [\"CVC-300\", \"CVC-ClinicDB\", \"CVC-ColonDB\", \"ETIS-LaribPolypDB\", \"Kvasir\"]\n",
    "test_dir = DATASET_ROOT / \"TestDataset\"\n",
    "\n",
    "for center in centers:\n",
    "    src = test_dir / center\n",
    "    dst = DATASET_ROOT / center\n",
    "    if src.exists() and not dst.exists():\n",
    "        print(f\"Organizing {center}...\")\n",
    "        shutil.copytree(str(src), str(dst))\n",
    "\n",
    "# Print statistics\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Dataset Statistics\")\n",
    "print(\"=\" * 50)\n",
    "for center in centers:\n",
    "    img_dir = DATASET_ROOT / center / \"images\"\n",
    "    if img_dir.exists():\n",
    "        n = len(list(img_dir.iterdir()))\n",
    "        print(f\"  {center:25s}: {n:4d} images\")\n",
    "    else:\n",
    "        print(f\"  {center:25s}: NOT FOUND\")\n",
    "\n",
    "print(\"\\nDataset setup complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Create project directory structure\n",
    "import sys\n",
    "\n",
    "PROJECT_ROOT = Path(\"/kaggle/working/GMLFNet\")\n",
    "for d in [\"data\", \"models\", \"trainers\", \"utils\", \"configs\"]:\n",
    "    (PROJECT_ROOT / d).mkdir(parents=True, exist_ok=True)\n",
    "    # Create __init__.py\n",
    "    (PROJECT_ROOT / d / \"__init__.py\").write_text(\"\")\n",
    "\n",
    "# Add project root to sys.path\n",
    "sys.path.insert(0, str(PROJECT_ROOT))\n",
    "print(f\"Project root: {PROJECT_ROOT}\")\n",
    "print(f\"Added to sys.path\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4a: Write data/augmentations.py\n",
    "augmentations_code = '''\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "\n",
    "def get_train_transforms(image_size=352):\n",
    "    \"\"\"Training augmentations for polyp segmentation.\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(image_size, image_size),\n",
    "        A.HorizontalFlip(p=0.5),\n",
    "        A.VerticalFlip(p=0.5),\n",
    "        A.RandomRotate90(p=0.5),\n",
    "        A.ColorJitter(\n",
    "            brightness=0.2, contrast=0.2,\n",
    "            saturation=0.2, hue=0.1, p=0.5\n",
    "        ),\n",
    "        A.GaussianBlur(blur_limit=(3, 7), p=0.3),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "\n",
    "\n",
    "def get_test_transforms(image_size=352):\n",
    "    \"\"\"Test/validation transforms (resize + normalize only).\"\"\"\n",
    "    return A.Compose([\n",
    "        A.Resize(image_size, image_size),\n",
    "        A.Normalize(\n",
    "            mean=[0.485, 0.456, 0.406],\n",
    "            std=[0.229, 0.224, 0.225],\n",
    "        ),\n",
    "        ToTensorV2(),\n",
    "    ])\n",
    "'''.strip()\n",
    "\n",
    "(PROJECT_ROOT / \"data\" / \"augmentations.py\").write_text(augmentations_code)\n",
    "print(\"Written: data/augmentations.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4b: Write data/datasets.py\n",
    "datasets_code = '''\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from .augmentations import get_train_transforms, get_test_transforms\n",
    "\n",
    "\n",
    "class PolypCenterDataset(Dataset):\n",
    "    \"\"\"Dataset for a single polyp segmentation center.\"\"\"\n",
    "\n",
    "    def __init__(self, root, center_name, transform=None, image_size=352):\n",
    "        self.root = Path(root)\n",
    "        self.center_name = center_name\n",
    "        self.image_size = image_size\n",
    "        self.transform = transform\n",
    "\n",
    "        self.image_dir = self.root / center_name / \"images\"\n",
    "        self.mask_dir = self.root / center_name / \"masks\"\n",
    "\n",
    "        if not self.image_dir.exists():\n",
    "            raise FileNotFoundError(f\"Image directory not found: {self.image_dir}\")\n",
    "        if not self.mask_dir.exists():\n",
    "            raise FileNotFoundError(f\"Mask directory not found: {self.mask_dir}\")\n",
    "\n",
    "        valid_exts = {\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\"}\n",
    "        self.image_files = sorted([\n",
    "            f for f in self.image_dir.iterdir()\n",
    "            if f.suffix.lower() in valid_exts\n",
    "        ])\n",
    "\n",
    "        if len(self.image_files) == 0:\n",
    "            raise RuntimeError(f\"No images found in {self.image_dir}\")\n",
    "\n",
    "        self.mask_files = []\n",
    "        for img_path in self.image_files:\n",
    "            mask_path = self._find_mask(img_path)\n",
    "            if mask_path is None:\n",
    "                raise FileNotFoundError(\n",
    "                    f\"No matching mask for {img_path.name} in {self.mask_dir}\"\n",
    "                )\n",
    "            self.mask_files.append(mask_path)\n",
    "\n",
    "    def _find_mask(self, img_path):\n",
    "        stem = img_path.stem\n",
    "        for ext in [\".png\", \".jpg\", \".jpeg\", \".bmp\", \".tif\"]:\n",
    "            mask_path = self.mask_dir / f\"{stem}{ext}\"\n",
    "            if mask_path.exists():\n",
    "                return mask_path\n",
    "        return None\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_files)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_files[idx]\n",
    "        mask_path = self.mask_files[idx]\n",
    "\n",
    "        image = cv2.imread(str(img_path))\n",
    "        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "        mask = cv2.imread(str(mask_path), cv2.IMREAD_GRAYSCALE)\n",
    "        mask = (mask > 128).astype(np.float32)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            transformed = self.transform(image=image, mask=mask)\n",
    "            image = transformed[\"image\"]\n",
    "            mask = transformed[\"mask\"]\n",
    "        else:\n",
    "            image = cv2.resize(image, (self.image_size, self.image_size))\n",
    "            mask = cv2.resize(mask, (self.image_size, self.image_size))\n",
    "            image = torch.from_numpy(image).permute(2, 0, 1).float() / 255.0\n",
    "            mask = torch.from_numpy(mask).float()\n",
    "\n",
    "        if mask.ndim == 2:\n",
    "            mask = mask.unsqueeze(0)\n",
    "\n",
    "        return {\n",
    "            \"image\": image,\n",
    "            \"mask\": mask,\n",
    "            \"center\": self.center_name,\n",
    "            \"filename\": img_path.name,\n",
    "        }\n",
    "\n",
    "\n",
    "def build_center_datasets(root, centers, transform=None, image_size=352):\n",
    "    datasets = {}\n",
    "    for center in centers:\n",
    "        datasets[center] = PolypCenterDataset(\n",
    "            root=root, center_name=center,\n",
    "            transform=transform, image_size=image_size,\n",
    "        )\n",
    "    return datasets\n",
    "'''.strip()\n",
    "\n",
    "(PROJECT_ROOT / \"data\" / \"datasets.py\").write_text(datasets_code)\n",
    "print(\"Written: data/datasets.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4c: Write data/meta_sampler.py\n",
    "meta_sampler_code = '''\n",
    "import random\n",
    "from collections import namedtuple\n",
    "from typing import Dict, List\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from .datasets import PolypCenterDataset\n",
    "\n",
    "\n",
    "Task = namedtuple(\"Task\", [\n",
    "    \"support_images\", \"support_masks\",\n",
    "    \"query_images\", \"query_masks\",\n",
    "    \"center_name\",\n",
    "])\n",
    "\n",
    "\n",
    "class CenterEpisodicSampler:\n",
    "    \"\"\"Creates meta-learning episodes where each task = one center.\"\"\"\n",
    "\n",
    "    def __init__(self, center_datasets, support_size=16, query_size=16, device=None):\n",
    "        self.center_datasets = center_datasets\n",
    "        self.support_size = support_size\n",
    "        self.query_size = query_size\n",
    "        self.device = device or torch.device(\"cpu\")\n",
    "        self.center_names = list(center_datasets.keys())\n",
    "\n",
    "        total_needed = support_size + query_size\n",
    "        for name, ds in center_datasets.items():\n",
    "            if len(ds) < total_needed:\n",
    "                print(f\"Warning: {name} has {len(ds)} images but \"\n",
    "                      f\"{total_needed} needed. Sampling with replacement.\")\n",
    "\n",
    "    def sample_episode(self):\n",
    "        tasks = []\n",
    "        for center_name in self.center_names:\n",
    "            task = self._sample_task(center_name)\n",
    "            tasks.append(task)\n",
    "        return tasks\n",
    "\n",
    "    def _sample_task(self, center_name):\n",
    "        dataset = self.center_datasets[center_name]\n",
    "        total_needed = self.support_size + self.query_size\n",
    "        n = len(dataset)\n",
    "\n",
    "        if n >= total_needed:\n",
    "            indices = random.sample(range(n), total_needed)\n",
    "        else:\n",
    "            indices = random.choices(range(n), k=total_needed)\n",
    "\n",
    "        support_indices = indices[:self.support_size]\n",
    "        query_indices = indices[self.support_size:]\n",
    "\n",
    "        support_images, support_masks = self._collate(dataset, support_indices)\n",
    "        query_images, query_masks = self._collate(dataset, query_indices)\n",
    "\n",
    "        return Task(\n",
    "            support_images=support_images.to(self.device),\n",
    "            support_masks=support_masks.to(self.device),\n",
    "            query_images=query_images.to(self.device),\n",
    "            query_masks=query_masks.to(self.device),\n",
    "            center_name=center_name,\n",
    "        )\n",
    "\n",
    "    def _collate(self, dataset, indices):\n",
    "        images, masks = [], []\n",
    "        for idx in indices:\n",
    "            sample = dataset[idx]\n",
    "            images.append(sample[\"image\"])\n",
    "            masks.append(sample[\"mask\"])\n",
    "        return torch.stack(images), torch.stack(masks)\n",
    "\n",
    "    def __len__(self):\n",
    "        max_size = max(len(ds) for ds in self.center_datasets.values())\n",
    "        return max(1, max_size // (self.support_size + self.query_size))\n",
    "'''.strip()\n",
    "\n",
    "(PROJECT_ROOT / \"data\" / \"meta_sampler.py\").write_text(meta_sampler_code)\n",
    "print(\"Written: data/meta_sampler.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4d: Write models/backbone.py\n",
    "backbone_code = '''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "try:\n",
    "    import timm\n",
    "except ImportError:\n",
    "    timm = None\n",
    "\n",
    "\n",
    "class Res2NetBackbone(nn.Module):\n",
    "    \"\"\"Res2Net-50 (v1b, 26w, 4s) encoder.\"\"\"\n",
    "    out_channels = [256, 512, 1024, 2048]\n",
    "\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        if timm is None:\n",
    "            raise ImportError(\"timm is required. Install: pip install timm\")\n",
    "        self.backbone = timm.create_model(\n",
    "            \"res2net50_26w_4s\", pretrained=pretrained,\n",
    "            features_only=True, out_indices=(1, 2, 3, 4),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "\n",
    "class PVTv2B2Backbone(nn.Module):\n",
    "    \"\"\"PVTv2-B2 transformer encoder.\"\"\"\n",
    "    out_channels = [64, 128, 320, 512]\n",
    "\n",
    "    def __init__(self, pretrained=True):\n",
    "        super().__init__()\n",
    "        if timm is None:\n",
    "            raise ImportError(\"timm is required. Install: pip install timm\")\n",
    "        self.backbone = timm.create_model(\n",
    "            \"pvt_v2_b2\", pretrained=pretrained,\n",
    "            features_only=True, out_indices=(0, 1, 2, 3),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.backbone(x)\n",
    "\n",
    "\n",
    "def get_backbone(name=\"res2net50\", pretrained=True):\n",
    "    if name == \"res2net50\":\n",
    "        return Res2NetBackbone(pretrained=pretrained)\n",
    "    elif name == \"pvt_v2_b2\":\n",
    "        return PVTv2B2Backbone(pretrained=pretrained)\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown backbone: {name}\")\n",
    "'''.strip()\n",
    "\n",
    "(PROJECT_ROOT / \"models\" / \"backbone.py\").write_text(backbone_code)\n",
    "print(\"Written: models/backbone.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4e: Write models/decoder.py\n",
    "decoder_code = '''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class RFB(nn.Module):\n",
    "    \"\"\"Receptive Field Block for multi-scale feature enhancement.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.branch0 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.branch1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=1, dilation=1),\n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.branch2 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=3, dilation=3),\n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.branch3 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, 3, padding=5, dilation=5),\n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.conv_cat = nn.Sequential(\n",
    "            nn.Conv2d(4 * out_channels, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.conv_res = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, 1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x0 = self.branch0(x)\n",
    "        x1 = self.branch1(x)\n",
    "        x2 = self.branch2(x)\n",
    "        x3 = self.branch3(x)\n",
    "        cat = torch.cat([x0, x1, x2, x3], dim=1)\n",
    "        out = self.conv_cat(cat) + self.conv_res(x)\n",
    "        return F.relu(out, inplace=True)\n",
    "\n",
    "\n",
    "class PartialDecoder(nn.Module):\n",
    "    \"\"\"Aggregates high-level features for initial prediction.\"\"\"\n",
    "\n",
    "    def __init__(self, channel):\n",
    "        super().__init__()\n",
    "        self.conv_upsample1 = nn.Sequential(\n",
    "            nn.Conv2d(channel, channel, 3, padding=1),\n",
    "            nn.BatchNorm2d(channel), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.conv_upsample2 = nn.Sequential(\n",
    "            nn.Conv2d(channel, channel, 3, padding=1),\n",
    "            nn.BatchNorm2d(channel), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.conv_concat = nn.Sequential(\n",
    "            nn.Conv2d(3 * channel, channel, 3, padding=1),\n",
    "            nn.BatchNorm2d(channel), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channel, channel, 3, padding=1),\n",
    "            nn.BatchNorm2d(channel), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.conv_out = nn.Conv2d(channel, 1, 1)\n",
    "\n",
    "    def forward(self, f2, f3, f4):\n",
    "        f4_up = F.interpolate(f4, size=f2.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        f4_up = self.conv_upsample1(f4_up)\n",
    "        f3_up = F.interpolate(f3, size=f2.shape[2:], mode=\"bilinear\", align_corners=False)\n",
    "        f3_up = self.conv_upsample2(f3_up)\n",
    "        cat = torch.cat([f2, f3_up, f4_up], dim=1)\n",
    "        fused = self.conv_concat(cat)\n",
    "        out = self.conv_out(fused)\n",
    "        return out, fused\n",
    "\n",
    "\n",
    "class ReverseAttention(nn.Module):\n",
    "    \"\"\"Reverse Attention module.\"\"\"\n",
    "\n",
    "    def __init__(self, in_channels, channel):\n",
    "        super().__init__()\n",
    "        self.conv_input = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, channel, 1),\n",
    "            nn.BatchNorm2d(channel), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.conv_refine = nn.Sequential(\n",
    "            nn.Conv2d(channel, channel, 3, padding=1),\n",
    "            nn.BatchNorm2d(channel), nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(channel, channel, 3, padding=1),\n",
    "            nn.BatchNorm2d(channel), nn.ReLU(inplace=True),\n",
    "        )\n",
    "        self.conv_out = nn.Conv2d(channel, 1, 1)\n",
    "\n",
    "    def forward(self, x, prev_pred):\n",
    "        reverse_mask = 1 - torch.sigmoid(prev_pred)\n",
    "        x = self.conv_input(x)\n",
    "        x = x * reverse_mask\n",
    "        x = self.conv_refine(x)\n",
    "        out = self.conv_out(x)\n",
    "        return out\n",
    "\n",
    "\n",
    "class MultiScaleDecoder(nn.Module):\n",
    "    \"\"\"Multi-scale decoder with RFB, Partial Decoder, and Reverse Attention.\"\"\"\n",
    "\n",
    "    def __init__(self, encoder_channels, decoder_channel=32):\n",
    "        super().__init__()\n",
    "        self.rfb2 = RFB(encoder_channels[1], decoder_channel)\n",
    "        self.rfb3 = RFB(encoder_channels[2], decoder_channel)\n",
    "        self.rfb4 = RFB(encoder_channels[3], decoder_channel)\n",
    "        self.partial_decoder = PartialDecoder(decoder_channel)\n",
    "        self.ra4 = ReverseAttention(encoder_channels[3], decoder_channel)\n",
    "        self.ra3 = ReverseAttention(encoder_channels[2], decoder_channel)\n",
    "        self.ra2 = ReverseAttention(encoder_channels[1], decoder_channel)\n",
    "\n",
    "    def forward(self, features, modulations=None):\n",
    "        f1, f2, f3, f4 = features\n",
    "        input_size = f1.shape[2] * 4, f1.shape[3] * 4\n",
    "\n",
    "        x2 = self.rfb2(f2)\n",
    "        x3 = self.rfb3(f3)\n",
    "        x4 = self.rfb4(f4)\n",
    "\n",
    "        if modulations is not None and len(modulations) >= 3:\n",
    "            gamma2, beta2 = modulations[0]\n",
    "            gamma3, beta3 = modulations[1]\n",
    "            gamma4, beta4 = modulations[2]\n",
    "            x2 = gamma2 * x2 + beta2\n",
    "            x3 = gamma3 * x3 + beta3\n",
    "            x4 = gamma4 * x4 + beta4\n",
    "\n",
    "        pred_init, fused = self.partial_decoder(x2, x3, x4)\n",
    "\n",
    "        side_preds = []\n",
    "        pred5 = F.interpolate(pred_init, size=input_size, mode=\"bilinear\", align_corners=False)\n",
    "        side_preds.append(pred5)\n",
    "\n",
    "        pred4 = self.ra4(f4, F.interpolate(pred_init, size=f4.shape[2:], mode=\"bilinear\", align_corners=False))\n",
    "        pred4_up = F.interpolate(pred4, size=input_size, mode=\"bilinear\", align_corners=False)\n",
    "        side_preds.append(pred4_up)\n",
    "\n",
    "        pred3 = self.ra3(f3, F.interpolate(pred4, size=f3.shape[2:], mode=\"bilinear\", align_corners=False))\n",
    "        pred3_up = F.interpolate(pred3, size=input_size, mode=\"bilinear\", align_corners=False)\n",
    "        side_preds.append(pred3_up)\n",
    "\n",
    "        pred2 = self.ra2(f2, F.interpolate(pred3, size=f2.shape[2:], mode=\"bilinear\", align_corners=False))\n",
    "        main_pred = F.interpolate(pred2, size=input_size, mode=\"bilinear\", align_corners=False)\n",
    "\n",
    "        return main_pred, side_preds\n",
    "'''.strip()\n",
    "\n",
    "(PROJECT_ROOT / \"models\" / \"decoder.py\").write_text(decoder_code)\n",
    "print(\"Written: models/decoder.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4f: Write models/fast_adapt_weights.py\n",
    "faw_code = '''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class FastAdaptationWeights(nn.Module):\n",
    "    \"\"\"Fast Adaptation Weights (FAW) module - thesis contribution.\n",
    "    \n",
    "    Generates per-layer FiLM modulation parameters (gamma, beta) that enable\n",
    "    rapid domain adaptation within the MAML inner loop.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, encoder_channels, num_modulation_layers=3,\n",
    "                 modulation_channels=32, hidden_dim=64, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.num_modulation_layers = num_modulation_layers\n",
    "        total_stats_dim = sum(encoder_channels)\n",
    "\n",
    "        layers = []\n",
    "        in_dim = total_stats_dim\n",
    "        for i in range(num_layers - 1):\n",
    "            layers.extend([nn.Linear(in_dim, hidden_dim), nn.ReLU(inplace=True)])\n",
    "            in_dim = hidden_dim\n",
    "\n",
    "        if num_layers == 1:\n",
    "            layers.extend([nn.Linear(total_stats_dim, hidden_dim), nn.ReLU(inplace=True)])\n",
    "\n",
    "        self.stats_encoder = nn.Sequential(*layers)\n",
    "\n",
    "        self.gamma_heads = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, modulation_channels)\n",
    "            for _ in range(num_modulation_layers)\n",
    "        ])\n",
    "        self.beta_heads = nn.ModuleList([\n",
    "            nn.Linear(hidden_dim, modulation_channels)\n",
    "            for _ in range(num_modulation_layers)\n",
    "        ])\n",
    "        self._init_identity()\n",
    "\n",
    "    def _init_identity(self):\n",
    "        for head in self.gamma_heads:\n",
    "            nn.init.zeros_(head.weight)\n",
    "            nn.init.ones_(head.bias)\n",
    "        for head in self.beta_heads:\n",
    "            nn.init.zeros_(head.weight)\n",
    "            nn.init.zeros_(head.bias)\n",
    "\n",
    "    def forward(self, encoder_features):\n",
    "        stats = []\n",
    "        for feat in encoder_features:\n",
    "            pooled = F.adaptive_avg_pool2d(feat, 1).flatten(1)\n",
    "            stats.append(pooled)\n",
    "        stats = torch.cat(stats, dim=1)\n",
    "\n",
    "        h = self.stats_encoder(stats)\n",
    "\n",
    "        modulations = []\n",
    "        for gamma_head, beta_head in zip(self.gamma_heads, self.beta_heads):\n",
    "            gamma = gamma_head(h).unsqueeze(-1).unsqueeze(-1)\n",
    "            beta = beta_head(h).unsqueeze(-1).unsqueeze(-1)\n",
    "            modulations.append((gamma, beta))\n",
    "\n",
    "        return modulations\n",
    "\n",
    "    def get_param_count(self):\n",
    "        return sum(p.numel() for p in self.parameters() if p.requires_grad)\n",
    "'''.strip()\n",
    "\n",
    "(PROJECT_ROOT / \"models\" / \"fast_adapt_weights.py\").write_text(faw_code)\n",
    "print(\"Written: models/fast_adapt_weights.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4g: Write models/gmlf_net.py\n",
    "gmlf_net_code = '''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from .backbone import get_backbone\n",
    "from .decoder import MultiScaleDecoder\n",
    "from .fast_adapt_weights import FastAdaptationWeights\n",
    "\n",
    "\n",
    "class GMLFNet(nn.Module):\n",
    "    \"\"\"GMLFNet segmentation model.\"\"\"\n",
    "\n",
    "    def __init__(self, backbone_name=\"res2net50\", decoder_channel=32,\n",
    "                 faw_hidden_dim=64, faw_num_layers=2,\n",
    "                 pretrained=True, use_faw=True):\n",
    "        super().__init__()\n",
    "        self.use_faw = use_faw\n",
    "\n",
    "        self.encoder = get_backbone(backbone_name, pretrained=pretrained)\n",
    "        enc_channels = self.encoder.out_channels\n",
    "\n",
    "        if use_faw:\n",
    "            self.faw = FastAdaptationWeights(\n",
    "                encoder_channels=enc_channels,\n",
    "                num_modulation_layers=3,\n",
    "                modulation_channels=decoder_channel,\n",
    "                hidden_dim=faw_hidden_dim,\n",
    "                num_layers=faw_num_layers,\n",
    "            )\n",
    "        else:\n",
    "            self.faw = None\n",
    "\n",
    "        self.decoder = MultiScaleDecoder(\n",
    "            encoder_channels=enc_channels,\n",
    "            decoder_channel=decoder_channel,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.encoder(x)\n",
    "        modulations = None\n",
    "        if self.use_faw and self.faw is not None:\n",
    "            modulations = self.faw(features)\n",
    "        main_pred, side_preds = self.decoder(features, modulations=modulations)\n",
    "        return main_pred, side_preds\n",
    "\n",
    "    def get_faw_parameters(self):\n",
    "        if self.faw is not None:\n",
    "            return list(self.faw.parameters())\n",
    "        return []\n",
    "\n",
    "    def get_non_faw_parameters(self):\n",
    "        return list(self.encoder.parameters()) + list(self.decoder.parameters())\n",
    "\n",
    "    def freeze_non_faw(self):\n",
    "        for param in self.encoder.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        for param in self.decoder.parameters():\n",
    "            param.requires_grad_(False)\n",
    "        if self.faw is not None:\n",
    "            for param in self.faw.parameters():\n",
    "                param.requires_grad_(True)\n",
    "\n",
    "    def unfreeze_all(self):\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad_(True)\n",
    "\n",
    "    def print_param_summary(self):\n",
    "        enc_params = sum(p.numel() for p in self.encoder.parameters())\n",
    "        dec_params = sum(p.numel() for p in self.decoder.parameters())\n",
    "        faw_params = self.faw.get_param_count() if self.faw else 0\n",
    "        total = enc_params + dec_params + faw_params\n",
    "        print(f\"Parameter Summary:\")\n",
    "        print(f\"  Encoder:  {enc_params:>10,}\")\n",
    "        print(f\"  Decoder:  {dec_params:>10,}\")\n",
    "        print(f\"  FAW:      {faw_params:>10,}\")\n",
    "        print(f\"  Total:    {total:>10,}\")\n",
    "        print(f\"  FAW ratio: {faw_params/total*100:.2f}% of total\")\n",
    "\n",
    "\n",
    "def build_model(cfg):\n",
    "    model = GMLFNet(\n",
    "        backbone_name=cfg.model.backbone,\n",
    "        decoder_channel=cfg.model.decoder_channels[-1] if hasattr(cfg.model, \"decoder_channels\") else 32,\n",
    "        faw_hidden_dim=cfg.model.faw_hidden_dim,\n",
    "        faw_num_layers=cfg.model.faw_num_layers,\n",
    "        pretrained=True,\n",
    "        use_faw=True,\n",
    "    )\n",
    "    return model\n",
    "'''.strip()\n",
    "\n",
    "(PROJECT_ROOT / \"models\" / \"gmlf_net.py\").write_text(gmlf_net_code)\n",
    "print(\"Written: models/gmlf_net.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4h: Write models/losses.py\n",
    "losses_code = '''\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class StructureLoss(nn.Module):\n",
    "    \"\"\"Structure-aware loss from PraNet.\"\"\"\n",
    "\n",
    "    def forward(self, pred, mask):\n",
    "        weit = 1 + 5 * torch.abs(\n",
    "            F.avg_pool2d(mask, kernel_size=31, stride=1, padding=15) - mask\n",
    "        )\n",
    "        wbce = F.binary_cross_entropy_with_logits(pred, mask, reduction=\"none\")\n",
    "        wbce = (weit * wbce).sum(dim=(2, 3)) / weit.sum(dim=(2, 3))\n",
    "\n",
    "        pred_sigmoid = torch.sigmoid(pred)\n",
    "        inter = ((pred_sigmoid * mask) * weit).sum(dim=(2, 3))\n",
    "        union = ((pred_sigmoid + mask) * weit).sum(dim=(2, 3))\n",
    "        wiou = 1 - (inter + 1) / (union - inter + 1)\n",
    "\n",
    "        return (wbce + wiou).mean()\n",
    "\n",
    "\n",
    "class BCEDiceLoss(nn.Module):\n",
    "    \"\"\"Combined BCE + Dice loss.\"\"\"\n",
    "\n",
    "    def __init__(self, bce_weight=0.5, dice_weight=0.5, smooth=1.0):\n",
    "        super().__init__()\n",
    "        self.bce_weight = bce_weight\n",
    "        self.dice_weight = dice_weight\n",
    "        self.smooth = smooth\n",
    "\n",
    "    def forward(self, pred, mask):\n",
    "        bce = F.binary_cross_entropy_with_logits(pred, mask)\n",
    "        pred_sigmoid = torch.sigmoid(pred)\n",
    "        inter = (pred_sigmoid * mask).sum(dim=(2, 3))\n",
    "        total = pred_sigmoid.sum(dim=(2, 3)) + mask.sum(dim=(2, 3))\n",
    "        dice = 1 - (2 * inter + self.smooth) / (total + self.smooth)\n",
    "        dice = dice.mean()\n",
    "        return self.bce_weight * bce + self.dice_weight * dice\n",
    "\n",
    "\n",
    "class GMLFNetLoss(nn.Module):\n",
    "    \"\"\"Composite loss with deep supervision for GMLFNet.\"\"\"\n",
    "\n",
    "    def __init__(self, structure_weight=1.0, side_weights=None):\n",
    "        super().__init__()\n",
    "        self.structure_loss = StructureLoss()\n",
    "        self.structure_weight = structure_weight\n",
    "        self.side_weights = side_weights or [0.5, 0.3, 0.2]\n",
    "\n",
    "    def forward(self, predictions, mask):\n",
    "        main_pred, side_preds = predictions\n",
    "\n",
    "        if main_pred.shape[2:] != mask.shape[2:]:\n",
    "            mask_resized = F.interpolate(\n",
    "                mask, size=main_pred.shape[2:],\n",
    "                mode=\"bilinear\", align_corners=False\n",
    "            )\n",
    "        else:\n",
    "            mask_resized = mask\n",
    "\n",
    "        total_loss = self.structure_weight * self.structure_loss(main_pred, mask_resized)\n",
    "\n",
    "        for i, side_pred in enumerate(side_preds):\n",
    "            weight = self.side_weights[i] if i < len(self.side_weights) else 0.1\n",
    "            if side_pred.shape[2:] != mask_resized.shape[2:]:\n",
    "                side_mask = F.interpolate(\n",
    "                    mask, size=side_pred.shape[2:],\n",
    "                    mode=\"bilinear\", align_corners=False\n",
    "                )\n",
    "            else:\n",
    "                side_mask = mask_resized\n",
    "            total_loss += weight * self.structure_loss(side_pred, side_mask)\n",
    "\n",
    "        return total_loss\n",
    "'''.strip()\n",
    "\n",
    "(PROJECT_ROOT / \"models\" / \"losses.py\").write_text(losses_code)\n",
    "print(\"Written: models/losses.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4i: Write utils/metrics.py\n",
    "metrics_code = '''\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "\n",
    "class SegmentationMetrics:\n",
    "    \"\"\"Accumulates predictions and computes segmentation metrics.\"\"\"\n",
    "\n",
    "    def __init__(self, threshold=0.5, beta_sq=0.3):\n",
    "        self.threshold = threshold\n",
    "        self.beta_sq = beta_sq\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.dice_scores = []\n",
    "        self.iou_scores = []\n",
    "        self.precision_scores = []\n",
    "        self.recall_scores = []\n",
    "        self.mae_scores = []\n",
    "        self.smeasure_scores = []\n",
    "        self.emeasure_scores = []\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def update(self, pred, mask):\n",
    "        pred = pred.cpu().numpy()\n",
    "        mask = mask.cpu().numpy()\n",
    "\n",
    "        for i in range(pred.shape[0]):\n",
    "            p = pred[i, 0]\n",
    "            m = mask[i, 0]\n",
    "            p_bin = (p >= self.threshold).astype(np.float32)\n",
    "            m_bin = m.astype(np.float32)\n",
    "\n",
    "            tp = (p_bin * m_bin).sum()\n",
    "            fp = (p_bin * (1 - m_bin)).sum()\n",
    "            fn = ((1 - p_bin) * m_bin).sum()\n",
    "\n",
    "            dice = (2 * tp + 1e-8) / (2 * tp + fp + fn + 1e-8)\n",
    "            self.dice_scores.append(dice)\n",
    "\n",
    "            iou = (tp + 1e-8) / (tp + fp + fn + 1e-8)\n",
    "            self.iou_scores.append(iou)\n",
    "\n",
    "            precision = (tp + 1e-8) / (tp + fp + 1e-8)\n",
    "            self.precision_scores.append(precision)\n",
    "\n",
    "            recall = (tp + 1e-8) / (tp + fn + 1e-8)\n",
    "            self.recall_scores.append(recall)\n",
    "\n",
    "            mae = np.abs(p - m_bin).mean()\n",
    "            self.mae_scores.append(mae)\n",
    "\n",
    "            sm = self._compute_smeasure(p, m_bin)\n",
    "            self.smeasure_scores.append(sm)\n",
    "\n",
    "            em = self._compute_emeasure(p_bin, m_bin)\n",
    "            self.emeasure_scores.append(em)\n",
    "\n",
    "    def compute(self):\n",
    "        n = len(self.dice_scores)\n",
    "        if n == 0:\n",
    "            return {\"dice\": 0.0, \"iou\": 0.0, \"precision\": 0.0,\n",
    "                    \"recall\": 0.0, \"fmeasure\": 0.0, \"mae\": 1.0,\n",
    "                    \"smeasure\": 0.0, \"emeasure\": 0.0}\n",
    "\n",
    "        precision = np.mean(self.precision_scores)\n",
    "        recall = np.mean(self.recall_scores)\n",
    "        fmeasure = ((1 + self.beta_sq) * precision * recall + 1e-8) / \\\\\n",
    "                   (self.beta_sq * precision + recall + 1e-8)\n",
    "\n",
    "        return {\n",
    "            \"dice\": float(np.mean(self.dice_scores)),\n",
    "            \"iou\": float(np.mean(self.iou_scores)),\n",
    "            \"precision\": float(precision),\n",
    "            \"recall\": float(recall),\n",
    "            \"fmeasure\": float(fmeasure),\n",
    "            \"mae\": float(np.mean(self.mae_scores)),\n",
    "            \"smeasure\": float(np.mean(self.smeasure_scores)),\n",
    "            \"emeasure\": float(np.mean(self.emeasure_scores)),\n",
    "        }\n",
    "\n",
    "    def _compute_smeasure(self, pred, mask, alpha=0.5):\n",
    "        y = mask.mean()\n",
    "        if y == 0:\n",
    "            return 1.0 - pred.mean()\n",
    "        elif y == 1:\n",
    "            return pred.mean()\n",
    "        else:\n",
    "            so = self._s_object(pred, mask)\n",
    "            sr = self._s_region(pred, mask)\n",
    "            return max(0.0, alpha * so + (1 - alpha) * sr)\n",
    "\n",
    "    def _s_object(self, pred, mask):\n",
    "        fg_pred = pred * mask\n",
    "        o_fg = self._object_score(fg_pred, mask)\n",
    "        bg_pred = (1 - pred) * (1 - mask)\n",
    "        o_bg = self._object_score(bg_pred, 1 - mask)\n",
    "        u = mask.mean()\n",
    "        return u * o_fg + (1 - u) * o_bg\n",
    "\n",
    "    def _object_score(self, pred, mask):\n",
    "        x = pred[mask > 0.5]\n",
    "        if len(x) == 0:\n",
    "            return 0.0\n",
    "        mu = x.mean()\n",
    "        std = x.std() + 1e-8\n",
    "        return 2 * mu / (mu ** 2 + 1 + std + 1e-8)\n",
    "\n",
    "    def _s_region(self, pred, mask):\n",
    "        h, w = mask.shape\n",
    "        cx, cy = h // 2, w // 2\n",
    "        score = 0.0\n",
    "        for si, sj in [(slice(0, cx), slice(0, cy)),\n",
    "                        (slice(0, cx), slice(cy, w)),\n",
    "                        (slice(cx, h), slice(0, cy)),\n",
    "                        (slice(cx, h), slice(cy, w))]:\n",
    "            p_region = pred[si, sj]\n",
    "            m_region = mask[si, sj]\n",
    "            weight = m_region.size / (h * w)\n",
    "            score += weight * self._ssim_like(p_region, m_region)\n",
    "        return score\n",
    "\n",
    "    def _ssim_like(self, pred, mask):\n",
    "        mu_p = pred.mean()\n",
    "        mu_m = mask.mean()\n",
    "        sigma_p = pred.std() + 1e-8\n",
    "        sigma_m = mask.std() + 1e-8\n",
    "        sigma_pm = ((pred - mu_p) * (mask - mu_m)).mean()\n",
    "        c1, c2 = 0.01 ** 2, 0.03 ** 2\n",
    "        luminance = (2 * mu_p * mu_m + c1) / (mu_p ** 2 + mu_m ** 2 + c1)\n",
    "        contrast = (2 * sigma_p * sigma_m + c2) / (sigma_p ** 2 + sigma_m ** 2 + c2)\n",
    "        structure = (sigma_pm + c2 / 2) / (sigma_p * sigma_m + c2 / 2)\n",
    "        return luminance * contrast * structure\n",
    "\n",
    "    def _compute_emeasure(self, pred_bin, mask):\n",
    "        if mask.sum() == 0 and pred_bin.sum() == 0:\n",
    "            return 1.0\n",
    "        if mask.sum() == 0 or pred_bin.sum() == 0:\n",
    "            return 0.0\n",
    "        mu_pred = pred_bin.mean()\n",
    "        mu_mask = mask.mean()\n",
    "        align_pred = pred_bin - mu_pred\n",
    "        align_mask = mask - mu_mask\n",
    "        align_matrix = 2 * (align_pred * align_mask) / \\\\\n",
    "                       (align_pred ** 2 + align_mask ** 2 + 1e-8)\n",
    "        enhanced = ((align_matrix + 1) ** 2) / 4\n",
    "        return enhanced.mean()\n",
    "'''.strip()\n",
    "\n",
    "(PROJECT_ROOT / \"utils\" / \"metrics.py\").write_text(metrics_code)\n",
    "print(\"Written: utils/metrics.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4j: Write utils/misc.py\n",
    "misc_code = '''\n",
    "import os\n",
    "import random\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import yaml\n",
    "\n",
    "\n",
    "class Config:\n",
    "    \"\"\"Simple nested config object from a dictionary.\"\"\"\n",
    "\n",
    "    def __init__(self, d):\n",
    "        for k, v in d.items():\n",
    "            if isinstance(v, dict):\n",
    "                setattr(self, k, Config(v))\n",
    "            elif isinstance(v, list):\n",
    "                setattr(self, k, [Config(i) if isinstance(i, dict) else i for i in v])\n",
    "            else:\n",
    "                setattr(self, k, v)\n",
    "\n",
    "    def __repr__(self):\n",
    "        return str(self.__dict__)\n",
    "\n",
    "    def to_dict(self):\n",
    "        result = {}\n",
    "        for k, v in self.__dict__.items():\n",
    "            if isinstance(v, Config):\n",
    "                result[k] = v.to_dict()\n",
    "            else:\n",
    "                result[k] = v\n",
    "        return result\n",
    "\n",
    "\n",
    "def load_config(config_path=\"configs/default.yaml\"):\n",
    "    with open(config_path, \"r\") as f:\n",
    "        cfg_dict = yaml.safe_load(f)\n",
    "    return Config(cfg_dict)\n",
    "\n",
    "\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "        torch.backends.cudnn.deterministic = True\n",
    "        torch.backends.cudnn.benchmark = False\n",
    "\n",
    "\n",
    "def get_device():\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "        print(f\"Using GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "        print(\"Using CPU\")\n",
    "    return device\n",
    "\n",
    "\n",
    "def save_checkpoint(model, optimizer, epoch, metrics, path):\n",
    "    path = Path(path)\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    state = {\n",
    "        \"epoch\": epoch,\n",
    "        \"model_state_dict\": model.state_dict(),\n",
    "        \"optimizer_state_dict\": optimizer.state_dict(),\n",
    "        \"metrics\": metrics,\n",
    "    }\n",
    "    torch.save(state, path)\n",
    "    print(f\"Checkpoint saved: {path}\")\n",
    "\n",
    "\n",
    "def load_checkpoint(model, optimizer=None, path=None):\n",
    "    if path is None or not Path(path).exists():\n",
    "        print(\"No checkpoint found, starting from scratch.\")\n",
    "        return 0, {}\n",
    "    checkpoint = torch.load(path, map_location=\"cpu\", weights_only=False)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    if optimizer is not None and \"optimizer_state_dict\" in checkpoint:\n",
    "        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "    epoch = checkpoint.get(\"epoch\", 0)\n",
    "    metrics = checkpoint.get(\"metrics\", {})\n",
    "    print(f\"Loaded checkpoint from epoch {epoch}\")\n",
    "    return epoch, metrics\n",
    "'''.strip()\n",
    "\n",
    "(PROJECT_ROOT / \"utils\" / \"misc.py\").write_text(misc_code)\n",
    "print(\"Written: utils/misc.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4k: Write trainers/meta_trainer.py\n",
    "meta_trainer_code = '''\n",
    "import time\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import learn2learn as l2l\n",
    "from tqdm import tqdm\n",
    "\n",
    "from data.meta_sampler import CenterEpisodicSampler\n",
    "from models.losses import GMLFNetLoss\n",
    "from utils.misc import save_checkpoint\n",
    "\n",
    "\n",
    "class MAMLMetaTrainer:\n",
    "    \"\"\"MAML meta-trainer for GMLFNet.\"\"\"\n",
    "\n",
    "    def __init__(self, model, sampler, cfg, device):\n",
    "        self.cfg = cfg\n",
    "        self.device = device\n",
    "        self.sampler = sampler\n",
    "\n",
    "        self.maml = l2l.algorithms.MAML(\n",
    "            model, lr=cfg.meta.inner_lr,\n",
    "            first_order=cfg.meta.first_order,\n",
    "            allow_unused=True, allow_nograd=True,\n",
    "        )\n",
    "        self.maml.to(device)\n",
    "\n",
    "        self.outer_optimizer = torch.optim.Adam(\n",
    "            self.maml.parameters(), lr=cfg.meta.outer_lr,\n",
    "        )\n",
    "\n",
    "        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(\n",
    "            self.outer_optimizer, T_max=cfg.training.epochs, eta_min=1e-6,\n",
    "        )\n",
    "\n",
    "        self.loss_fn = GMLFNetLoss()\n",
    "        self.inner_steps = cfg.meta.inner_steps\n",
    "        self.selective_adaptation = True\n",
    "\n",
    "    def meta_train_step(self):\n",
    "        self.outer_optimizer.zero_grad()\n",
    "        meta_loss = 0.0\n",
    "        task_losses = {}\n",
    "\n",
    "        episode = self.sampler.sample_episode()\n",
    "\n",
    "        for task in episode:\n",
    "            learner = self.maml.clone()\n",
    "\n",
    "            if self.selective_adaptation:\n",
    "                for name, param in learner.named_parameters():\n",
    "                    if \"faw\" not in name:\n",
    "                        param.requires_grad_(False)\n",
    "                    else:\n",
    "                        param.requires_grad_(True)\n",
    "\n",
    "            for step in range(self.inner_steps):\n",
    "                support_pred = learner(task.support_images)\n",
    "                support_loss = self.loss_fn(support_pred, task.support_masks)\n",
    "                learner.adapt(support_loss)\n",
    "\n",
    "            if self.selective_adaptation:\n",
    "                for param in learner.parameters():\n",
    "                    param.requires_grad_(True)\n",
    "\n",
    "            query_pred = learner(task.query_images)\n",
    "            query_loss = self.loss_fn(query_pred, task.query_masks)\n",
    "            meta_loss += query_loss\n",
    "            task_losses[task.center_name] = query_loss.item()\n",
    "\n",
    "        meta_loss /= len(episode)\n",
    "        meta_loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(\n",
    "            self.maml.parameters(), self.cfg.training.grad_clip,\n",
    "        )\n",
    "        self.outer_optimizer.step()\n",
    "\n",
    "        return meta_loss.item(), task_losses\n",
    "\n",
    "    def train_epoch(self, epoch):\n",
    "        self.maml.train()\n",
    "        steps_per_epoch = len(self.sampler)\n",
    "        total_loss = 0.0\n",
    "\n",
    "        pbar = tqdm(range(steps_per_epoch), desc=f\"Epoch {epoch}\")\n",
    "        for step in pbar:\n",
    "            loss, task_losses = self.meta_train_step()\n",
    "            total_loss += loss\n",
    "            task_str = \" | \".join(\n",
    "                f\"{k[:8]}:{v:.4f}\" for k, v in task_losses.items()\n",
    "            )\n",
    "            pbar.set_postfix_str(f\"loss={loss:.4f} | {task_str}\")\n",
    "\n",
    "        self.scheduler.step()\n",
    "        return total_loss / steps_per_epoch\n",
    "\n",
    "    def train(self, evaluator=None, logger=None):\n",
    "        best_dice = 0.0\n",
    "        start_epoch = 0\n",
    "\n",
    "        if self.cfg.training.resume:\n",
    "            checkpoint = torch.load(\n",
    "                self.cfg.training.resume,\n",
    "                map_location=self.device, weights_only=False,\n",
    "            )\n",
    "            self.maml.module.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "            self.outer_optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n",
    "            start_epoch = checkpoint.get(\"epoch\", 0) + 1\n",
    "            best_dice = checkpoint.get(\"metrics\", {}).get(\"mean_dice\", 0.0)\n",
    "            print(f\"Resumed from epoch {start_epoch}, best_dice={best_dice:.4f}\")\n",
    "\n",
    "        for epoch in range(start_epoch, self.cfg.training.epochs):\n",
    "            t0 = time.time()\n",
    "            avg_loss = self.train_epoch(epoch)\n",
    "            elapsed = time.time() - t0\n",
    "\n",
    "            print(f\"Epoch {epoch}: avg_loss={avg_loss:.4f}, time={elapsed:.1f}s, \"\n",
    "                  f\"lr={self.scheduler.get_last_lr()[0]:.6f}\")\n",
    "\n",
    "            if evaluator and (epoch + 1) % self.cfg.logging.save_interval == 0:\n",
    "                results = evaluator.full_evaluation(self.maml)\n",
    "                mean_dice = sum(\n",
    "                    r[\"dice\"] for r in results.values()\n",
    "                ) / len(results)\n",
    "\n",
    "                print(f\"  Eval mean_dice={mean_dice:.4f}\")\n",
    "                for center, metrics in results.items():\n",
    "                    print(f\"    {center}: dice={metrics[\\\"dice\\\"]:.4f}, \"\n",
    "                          f\"iou={metrics[\\\"iou\\\"]:.4f}\")\n",
    "\n",
    "                if mean_dice > best_dice:\n",
    "                    best_dice = mean_dice\n",
    "                    save_checkpoint(\n",
    "                        self.maml.module, self.outer_optimizer, epoch,\n",
    "                        {\"mean_dice\": mean_dice, **results},\n",
    "                        Path(self.cfg.logging.log_dir) / \"best_model.pth\",\n",
    "                    )\n",
    "\n",
    "            if (epoch + 1) % self.cfg.logging.save_interval == 0:\n",
    "                save_checkpoint(\n",
    "                    self.maml.module, self.outer_optimizer, epoch,\n",
    "                    {\"avg_loss\": avg_loss},\n",
    "                    Path(self.cfg.logging.log_dir) / f\"checkpoint_epoch{epoch}.pth\",\n",
    "                )\n",
    "\n",
    "        print(f\"\\\\nTraining complete. Best mean Dice: {best_dice:.4f}\")\n",
    "'''.strip()\n",
    "\n",
    "(PROJECT_ROOT / \"trainers\" / \"meta_trainer.py\").write_text(meta_trainer_code)\n",
    "print(\"Written: trainers/meta_trainer.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4l: Write trainers/evaluator.py\n",
    "evaluator_code = '''\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import learn2learn as l2l\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from utils.metrics import SegmentationMetrics\n",
    "from models.losses import GMLFNetLoss\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\"Evaluator for polyp segmentation models.\"\"\"\n",
    "\n",
    "    def __init__(self, test_datasets, cfg, device):\n",
    "        self.test_datasets = test_datasets\n",
    "        self.cfg = cfg\n",
    "        self.device = device\n",
    "        self.loss_fn = GMLFNetLoss()\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def evaluate_zero_shot(self, model, center_name):\n",
    "        if isinstance(model, l2l.algorithms.MAML):\n",
    "            eval_model = model.module\n",
    "        else:\n",
    "            eval_model = model\n",
    "\n",
    "        eval_model.eval()\n",
    "        dataset = self.test_datasets[center_name]\n",
    "        loader = DataLoader(dataset, batch_size=1, shuffle=False)\n",
    "        metrics = SegmentationMetrics()\n",
    "\n",
    "        for batch in loader:\n",
    "            image = batch[\"image\"].to(self.device)\n",
    "            mask = batch[\"mask\"].to(self.device)\n",
    "            main_pred, _ = eval_model(image)\n",
    "            pred = torch.sigmoid(main_pred)\n",
    "            if pred.shape[2:] != mask.shape[2:]:\n",
    "                pred = F.interpolate(\n",
    "                    pred, size=mask.shape[2:],\n",
    "                    mode=\"bilinear\", align_corners=False,\n",
    "                )\n",
    "            metrics.update(pred, mask)\n",
    "\n",
    "        return metrics.compute()\n",
    "\n",
    "    def evaluate_few_shot(self, maml_model, center_name,\n",
    "                          k_support=5, adaptation_steps=5):\n",
    "        dataset = self.test_datasets[center_name]\n",
    "        n = len(dataset)\n",
    "\n",
    "        if n <= k_support:\n",
    "            print(f\"Warning: {center_name} has only {n} images\")\n",
    "            return self.evaluate_zero_shot(maml_model, center_name)\n",
    "\n",
    "        indices = list(range(n))\n",
    "        support_indices = indices[:k_support]\n",
    "        query_indices = indices[k_support:]\n",
    "\n",
    "        support_images = []\n",
    "        support_masks = []\n",
    "        for idx in support_indices:\n",
    "            sample = dataset[idx]\n",
    "            support_images.append(sample[\"image\"])\n",
    "            support_masks.append(sample[\"mask\"])\n",
    "        support_images = torch.stack(support_images).to(self.device)\n",
    "        support_masks = torch.stack(support_masks).to(self.device)\n",
    "\n",
    "        learner = maml_model.clone()\n",
    "        for name, param in learner.named_parameters():\n",
    "            if \"faw\" not in name:\n",
    "                param.requires_grad_(False)\n",
    "            else:\n",
    "                param.requires_grad_(True)\n",
    "\n",
    "        for step in range(adaptation_steps):\n",
    "            pred = learner(support_images)\n",
    "            loss = self.loss_fn(pred, support_masks)\n",
    "            learner.adapt(loss)\n",
    "\n",
    "        learner.eval()\n",
    "        metrics = SegmentationMetrics()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for idx in query_indices:\n",
    "                sample = dataset[idx]\n",
    "                image = sample[\"image\"].unsqueeze(0).to(self.device)\n",
    "                mask = sample[\"mask\"].unsqueeze(0).to(self.device)\n",
    "                main_pred, _ = learner(image)\n",
    "                pred = torch.sigmoid(main_pred)\n",
    "                if pred.shape[2:] != mask.shape[2:]:\n",
    "                    pred = F.interpolate(\n",
    "                        pred, size=mask.shape[2:],\n",
    "                        mode=\"bilinear\", align_corners=False,\n",
    "                    )\n",
    "                metrics.update(pred, mask)\n",
    "\n",
    "        return metrics.compute()\n",
    "\n",
    "    def full_evaluation(self, model, mode=\"zero_shot\"):\n",
    "        results = {}\n",
    "        for center_name in self.test_datasets:\n",
    "            if mode == \"few_shot\" and isinstance(model, l2l.algorithms.MAML):\n",
    "                results[center_name] = self.evaluate_few_shot(model, center_name)\n",
    "            else:\n",
    "                results[center_name] = self.evaluate_zero_shot(model, center_name)\n",
    "        return results\n",
    "'''.strip()\n",
    "\n",
    "(PROJECT_ROOT / \"trainers\" / \"evaluator.py\").write_text(evaluator_code)\n",
    "print(\"Written: trainers/evaluator.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4m: Verify all modules are importable\n",
    "print(\"Verifying module imports...\")\n",
    "\n",
    "# Force reimport\n",
    "import importlib\n",
    "for mod_name in list(sys.modules.keys()):\n",
    "    if 'data.' in mod_name or 'models.' in mod_name or 'trainers.' in mod_name or 'utils.' in mod_name:\n",
    "        del sys.modules[mod_name]\n",
    "\n",
    "from data.augmentations import get_train_transforms, get_test_transforms\n",
    "from data.datasets import PolypCenterDataset, build_center_datasets\n",
    "from data.meta_sampler import CenterEpisodicSampler\n",
    "from models.backbone import get_backbone\n",
    "from models.decoder import MultiScaleDecoder\n",
    "from models.fast_adapt_weights import FastAdaptationWeights\n",
    "from models.gmlf_net import GMLFNet, build_model\n",
    "from models.losses import GMLFNetLoss, StructureLoss\n",
    "from utils.metrics import SegmentationMetrics\n",
    "from utils.misc import Config, set_seed, get_device, save_checkpoint\n",
    "\n",
    "print(\"All modules imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Configuration\n",
    "import yaml\n",
    "\n",
    "config_dict = {\n",
    "    \"data\": {\n",
    "        \"root\": \"/kaggle/working/datasets\",\n",
    "        \"image_size\": 352,\n",
    "        \"train_centers\": [\"Kvasir\", \"CVC-ClinicDB\", \"CVC-ColonDB\"],\n",
    "        \"test_centers\": [\"ETIS-LaribPolypDB\", \"CVC-300\"],\n",
    "        \"num_workers\": 2,\n",
    "        \"pin_memory\": True,\n",
    "    },\n",
    "    \"model\": {\n",
    "        \"backbone\": \"res2net50\",\n",
    "        \"decoder_channels\": [256, 128, 64, 32],\n",
    "        \"faw_hidden_dim\": 64,\n",
    "        \"faw_num_layers\": 2,\n",
    "    },\n",
    "    \"meta\": {\n",
    "        \"algorithm\": \"maml\",\n",
    "        \"inner_lr\": 0.01,\n",
    "        \"inner_steps\": 5,\n",
    "        \"outer_lr\": 0.001,\n",
    "        \"tasks_per_batch\": 3,\n",
    "        \"support_size\": 16,\n",
    "        \"query_size\": 16,\n",
    "        \"first_order\": True,\n",
    "    },\n",
    "    \"training\": {\n",
    "        \"epochs\": 200,\n",
    "        \"seed\": 42,\n",
    "        \"grad_clip\": 1.0,\n",
    "        \"scheduler\": \"cosine\",\n",
    "        \"warmup_epochs\": 10,\n",
    "        \"resume\": \"\",\n",
    "    },\n",
    "    \"loss\": {\n",
    "        \"bce_weight\": 0.5,\n",
    "        \"dice_weight\": 0.5,\n",
    "        \"structure_weight\": 0.2,\n",
    "    },\n",
    "    \"logging\": {\n",
    "        \"backend\": \"tensorboard\",\n",
    "        \"log_dir\": \"/kaggle/working/runs\",\n",
    "        \"save_interval\": 10,\n",
    "        \"wandb_project\": \"GMLFNet\",\n",
    "    },\n",
    "}\n",
    "\n",
    "# Save config\n",
    "config_path = PROJECT_ROOT / \"configs\" / \"default.yaml\"\n",
    "with open(config_path, \"w\") as f:\n",
    "    yaml.dump(config_dict, f, default_flow_style=False)\n",
    "\n",
    "cfg = Config(config_dict)\n",
    "print(\"Configuration loaded:\")\n",
    "print(f\"  Backbone: {cfg.model.backbone}\")\n",
    "print(f\"  Train centers: {cfg.data.train_centers}\")\n",
    "print(f\"  Test centers: {cfg.data.test_centers}\")\n",
    "print(f\"  Inner LR: {cfg.meta.inner_lr}, Inner Steps: {cfg.meta.inner_steps}\")\n",
    "print(f\"  Outer LR: {cfg.meta.outer_lr}\")\n",
    "print(f\"  Epochs: {cfg.training.epochs}\")\n",
    "print(f\"  FOMAML: {cfg.meta.first_order}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Build Model & Verify\n",
    "import torch\n",
    "\n",
    "set_seed(cfg.training.seed)\n",
    "device = get_device()\n",
    "\n",
    "model = build_model(cfg)\n",
    "model.print_param_summary()\n",
    "\n",
    "# Sanity check: forward pass with dummy input\n",
    "print(\"\\nRunning sanity check...\")\n",
    "dummy_input = torch.randn(2, 3, 352, 352).to(device)\n",
    "model = model.to(device)\n",
    "with torch.no_grad():\n",
    "    main_pred, side_preds = model(dummy_input)\n",
    "print(f\"  Input shape:  {dummy_input.shape}\")\n",
    "print(f\"  Output shape: {main_pred.shape}\")\n",
    "print(f\"  Side outputs: {len(side_preds)} (shapes: {[s.shape for s in side_preds]})\")\n",
    "print(\"Sanity check passed!\")\n",
    "\n",
    "# Move back to CPU for MAML wrapping\n",
    "model = model.cpu()\n",
    "del dummy_input\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Build Datasets & Sampler\n",
    "print(\"Building datasets...\")\n",
    "\n",
    "train_transform = get_train_transforms(cfg.data.image_size)\n",
    "test_transform = get_test_transforms(cfg.data.image_size)\n",
    "\n",
    "train_datasets = build_center_datasets(\n",
    "    root=cfg.data.root,\n",
    "    centers=cfg.data.train_centers,\n",
    "    transform=train_transform,\n",
    "    image_size=cfg.data.image_size,\n",
    ")\n",
    "\n",
    "test_datasets = build_center_datasets(\n",
    "    root=cfg.data.root,\n",
    "    centers=cfg.data.test_centers,\n",
    "    transform=test_transform,\n",
    "    image_size=cfg.data.image_size,\n",
    ")\n",
    "\n",
    "print(\"\\nTraining datasets:\")\n",
    "for name, ds in train_datasets.items():\n",
    "    print(f\"  {name}: {len(ds)} images\")\n",
    "\n",
    "print(\"\\nTest datasets:\")\n",
    "for name, ds in test_datasets.items():\n",
    "    print(f\"  {name}: {len(ds)} images\")\n",
    "\n",
    "# Create episodic sampler\n",
    "sampler = CenterEpisodicSampler(\n",
    "    center_datasets=train_datasets,\n",
    "    support_size=cfg.meta.support_size,\n",
    "    query_size=cfg.meta.query_size,\n",
    "    device=device,\n",
    ")\n",
    "print(f\"\\nEpisodic sampler: {len(sampler)} episodes/epoch\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8: Train (Meta-Learning)\n",
    "from trainers.meta_trainer import MAMLMetaTrainer\n",
    "from trainers.evaluator import Evaluator\n",
    "\n",
    "print(\"Initializing MAML meta-trainer...\")\n",
    "\n",
    "trainer = MAMLMetaTrainer(\n",
    "    model=model,\n",
    "    sampler=sampler,\n",
    "    cfg=cfg,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "evaluator = Evaluator(\n",
    "    test_datasets=test_datasets,\n",
    "    cfg=cfg,\n",
    "    device=device,\n",
    ")\n",
    "\n",
    "print(\"Starting meta-learning training...\")\n",
    "print(f\"  Epochs: {cfg.training.epochs}\")\n",
    "print(f\"  Inner steps: {cfg.meta.inner_steps}\")\n",
    "print(f\"  Inner LR: {cfg.meta.inner_lr}\")\n",
    "print(f\"  Outer LR: {cfg.meta.outer_lr}\")\n",
    "print(f\"  FOMAML: {cfg.meta.first_order}\")\n",
    "print(f\"  Selective adaptation (FAW only): True\")\n",
    "print(f\"  Eval interval: every {cfg.logging.save_interval} epochs\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "trainer.train(evaluator=evaluator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9: Final Evaluation\n",
    "import json\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"FINAL EVALUATION\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Load best model\n",
    "best_path = Path(cfg.logging.log_dir) / \"best_model.pth\"\n",
    "if best_path.exists():\n",
    "    checkpoint = torch.load(best_path, map_location=device, weights_only=False)\n",
    "    model.load_state_dict(checkpoint[\"model_state_dict\"])\n",
    "    print(f\"Loaded best model from epoch {checkpoint['epoch']}\")\n",
    "    print(f\"Best mean Dice during training: {checkpoint['metrics'].get('mean_dice', 'N/A')}\")\n",
    "else:\n",
    "    print(\"No best model checkpoint found, using current model.\")\n",
    "\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "# Evaluate on all centers (train + test)\n",
    "all_centers = cfg.data.train_centers + cfg.data.test_centers\n",
    "all_datasets = {**train_datasets, **test_datasets}\n",
    "\n",
    "all_evaluator = Evaluator(test_datasets=all_datasets, cfg=cfg, device=device)\n",
    "\n",
    "print(\"\\n--- Zero-Shot Evaluation ---\")\n",
    "results = {}\n",
    "for center in all_centers:\n",
    "    metrics = all_evaluator.evaluate_zero_shot(model, center)\n",
    "    results[center] = metrics\n",
    "\n",
    "# Print formatted table\n",
    "print(f\"\\n{'Center':25s} | {'Dice':>7s} | {'IoU':>7s} | {'F-meas':>7s} | {'MAE':>7s} | {'S-meas':>7s} | {'E-meas':>7s}\")\n",
    "print(\"-\" * 90)\n",
    "for center, m in results.items():\n",
    "    marker = \" *\" if center in cfg.data.test_centers else \"\"\n",
    "    print(f\"{center + marker:25s} | {m['dice']:7.4f} | {m['iou']:7.4f} | {m['fmeasure']:7.4f} | \"\n",
    "          f\"{m['mae']:7.4f} | {m['smeasure']:7.4f} | {m['emeasure']:7.4f}\")\n",
    "\n",
    "# Compute mean over test centers\n",
    "test_results = {k: v for k, v in results.items() if k in cfg.data.test_centers}\n",
    "mean_dice = sum(m['dice'] for m in test_results.values()) / len(test_results)\n",
    "mean_iou = sum(m['iou'] for m in test_results.values()) / len(test_results)\n",
    "print(f\"\\nMean over test centers: Dice={mean_dice:.4f}, IoU={mean_iou:.4f}\")\n",
    "print(\"\\n* = unseen test center (zero-shot generalization)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10: Save Results to Kaggle Output\n",
    "output_dir = Path(\"/kaggle/working\")\n",
    "\n",
    "# Save evaluation results\n",
    "results_path = output_dir / \"evaluation_results.json\"\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "print(f\"Results saved to: {results_path}\")\n",
    "\n",
    "# Copy best model to output\n",
    "if best_path.exists():\n",
    "    output_model = output_dir / \"best_model.pth\"\n",
    "    shutil.copy2(best_path, output_model)\n",
    "    print(f\"Best model copied to: {output_model}\")\n",
    "\n",
    "# List all checkpoints\n",
    "runs_dir = Path(cfg.logging.log_dir)\n",
    "if runs_dir.exists():\n",
    "    checkpoints = list(runs_dir.glob(\"*.pth\"))\n",
    "    print(f\"\\nAll checkpoints in {runs_dir}:\")\n",
    "    for cp in sorted(checkpoints):\n",
    "        size_mb = cp.stat().st_size / (1024 * 1024)\n",
    "        print(f\"  {cp.name}: {size_mb:.1f} MB\")\n",
    "\n",
    "print(\"\\nDone! Download results from the Output tab.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}